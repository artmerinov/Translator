{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'it': 'Anna Karenina', 'ru': 'Анна Каренина'},\n",
       " {'it': 'Lev Tolstoj', 'ru': 'Толстой Лев Николаевич'},\n",
       " {'it': 'PARTE PRIMA', 'ru': 'ЧАСТЬ ПЕРВАЯ'},\n",
       " {'it': 'I', 'ru': 'I'},\n",
       " {'it': 'Tutte le famiglie felici sono simili le une alle altre; ogni famiglia infelice è infelice a modo suo.',\n",
       "  'ru': 'Все счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.'},\n",
       " {'it': 'Tutto era sottosopra in casa Oblonskij.',\n",
       "  'ru': 'Все смешалось в доме Облонских.'},\n",
       " {'it': 'La moglie era venuta a sapere che il marito aveva una relazione con la governante francese che era stata presso di loro, e aveva dichiarato al marito di non poter più vivere con lui nella stessa casa.',\n",
       "  'ru': 'Жена узнала, что муж был в связи с бывшею в их доме француженкою-гувернанткой, и объявила мужу, что не может жить с ним в одном доме.'},\n",
       " {'it': 'Questa situazione durava già da tre giorni ed era sentita tormentosamente dagli stessi coniugi e da tutti i membri della famiglia e dai domestici.',\n",
       "  'ru': 'Положение это продолжалось уже третий день и мучительно чувствовалось и самими супругами, и всеми членами семьи, и домочадцами.'},\n",
       " {'it': 'Tutti i membri della famiglia e i domestici sentivano che non c’era senso nella loro convivenza, e che della gente incontratasi per caso in una qualsiasi locanda sarebbe stata più legata fra di sé che non loro, membri della famiglia e domestici degli Oblonskij.',\n",
       "  'ru': 'Все члены семьи и домочадцы чувствовали, что нет смысла в их сожительстве и что на каждом постоялом дворе случайно сошедшиеся люди более связаны между собой, чем они, члены семьи и домочадцы Облонских.'},\n",
       " {'it': 'La moglie non usciva dalle sue stanze; il marito era già il terzo giorno che non rincasava.',\n",
       "  'ru': 'Жена не выходила из своих комнат, мужа третий день не было дома.'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = load_dataset(\n",
    "    path='opus_books', \n",
    "    name='it-ru',\n",
    "    split='train'\n",
    ")\n",
    "data_raw['translation'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_all_sentences(dataset, lang: str):\n",
    "    \"\"\"\n",
    "    Iterator of sentences.\n",
    "    \"\"\"\n",
    "    for item in dataset[\"translation\"]:\n",
    "        yield item[lang]\n",
    "\n",
    "\n",
    "def _create_tokenizer(dataset, lang: str, algorithm: str = 'wordlevel') -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Creates tokenizer from text file and saves it.\n",
    "    \"\"\"\n",
    "    unk_token = \"[UNK]\"\n",
    "    special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"]\n",
    "\n",
    "    if algorithm == 'bpe':\n",
    "        model = BPE(unk_token=unk_token)\n",
    "        trainer = BpeTrainer(special_tokens=special_tokens, min_frequency=5)\n",
    "    else:\n",
    "        model = WordLevel(unk_token=unk_token)\n",
    "        trainer = WordLevelTrainer(special_tokens=special_tokens)\n",
    "    \n",
    "    tokenizer = Tokenizer(model=model)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.train_from_iterator(iterator=_get_all_sentences(dataset, lang), trainer=trainer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def make_tokenizer(dataset, lang: str, algorithm: str):\n",
    "    \"\"\"\n",
    "    Create BPE (Byte-Pair Encoding) tokenizer.\n",
    "    \"\"\"\n",
    "    # make sure that the tokenizer folder exists\n",
    "    if not os.path.exists('tokenizers'):\n",
    "        os.mkdir('tokenizers')\n",
    "\n",
    "    # path to the tokenizer file\n",
    "    tokenizer_path = os.path.join('tokenizers', f'tokenizer_{algorithm}_{lang}.json')\n",
    "    \n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        tokenizer = _create_tokenizer(dataset=dataset, lang=lang, algorithm=algorithm)\n",
    "        tokenizer.save(tokenizer_path)\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = make_tokenizer(dataset=data_raw, lang='ru', algorithm='bpe')\n",
    "tgt_tokenizer = make_tokenizer(dataset=data_raw, lang='it', algorithm='bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Все', 'счастли', 'вые', 'семьи', 'похожи', 'друг', 'на', 'друга', ',', 'каждая', 'не', 'счастливая', 'семья', 'несчастлива', 'по', '-', 'своему', '.']\n",
      "['T', 'u', 'tte', 'le', 'f', 'am', 'i', 'g', 'l', 'ie', 'f', 'e', 'l', 'ic', 'i', 's', 'on', 'o', 'si', 'm', 'il', 'i', 'le', 'une', 'a', 'lle', 'a', 'l', 't', 're', ';', 'o', 'g', 'ni', 'f', 'am', 'i', 'g', 'l', 'i', 'a', 'in', 'f', 'e', 'l', 'i', 'ce', '[UNK]', 'in', 'f', 'e', 'l', 'i', 'ce', 'a', 'mo', 'd', 'o', 's', 'u', 'o', '.']\n"
     ]
    }
   ],
   "source": [
    "src_string = 'Все счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.'\n",
    "tgt_string = 'Tutte le famiglie felici sono simili le une alle altre; ogni famiglia infelice è infelice a modo suo.'\n",
    "\n",
    "src_output = src_tokenizer.encode(src_string)\n",
    "print(src_output.tokens)\n",
    "\n",
    "tgt_output = src_tokenizer.encode(tgt_string)\n",
    "print(tgt_output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
